{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WClP0jSbx2MC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "fiona & jay's attempt to predict german credit data... don't give us any loans"
      ],
      "metadata": {
        "id": "L0082VxLR18k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1-3: Create train/test sets"
      ],
      "metadata": {
        "id": "WClP0jSbx2MC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0-1"
      ],
      "metadata": {
        "id": "9SzC-g6klCWk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OxZRNeAoq9TJ"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "# STEP 0: import libraries\n",
        "########################################\n",
        "import pandas as pd\n",
        "import sklearn.datasets\n",
        "import sklearn.decomposition\n",
        "import sklearn.discriminant_analysis\n",
        "import sklearn.ensemble\n",
        "import sklearn.linear_model\n",
        "import sklearn.neural_network\n",
        "import sklearn.model_selection\n",
        "import sklearn.naive_bayes\n",
        "import sklearn.neighbors\n",
        "import sklearn.preprocessing\n",
        "import sklearn.random_projection\n",
        "import sklearn.tree\n",
        "import sklearn.svm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# STEP 1: Load the dataset\n",
        "########################################\n",
        "\n",
        "# NOTE:\n",
        "# This code is problem specific.\n",
        "# All the other code in this file is generic to work with any dataset.\n",
        "# The \"German Credit\" dataset is a famous dataset for predicting whether someone will default on a loan.\n",
        "# It has been used in previous practicum projects for banks.\n",
        "# It is a publicly accessible dataset that is similar in format/content to their private datasets.\n",
        "data = sklearn.datasets.fetch_openml(name='GermanCreditData')\n",
        "df = data.data\n",
        "target = data.target\n",
        "print(f\"df.shape={df.shape}\")\n",
        "print(f\"target.shape={target.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCM8HhACrPs_",
        "outputId": "cadf3d6e-3d5b-4977-e20d-6f358656b607"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df.shape=(1000, 21)\n",
            "target.shape=(1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2"
      ],
      "metadata": {
        "id": "T7roFm2ZlEkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ########################################\n",
        "# # STEP 2: Apply \"non-learned\" data transformations\n",
        "# ########################################\n",
        "\n",
        "\n",
        "# convert non-numeric columns to numeric\n",
        "le = sklearn.preprocessing.LabelEncoder()\n",
        "df = df[df.columns[:]].apply(le.fit_transform)\n",
        "print(f\"df.shape={df.shape}\")\n",
        "\n",
        "# # # apply the polynomial feature map\n",
        "# # poly = sklearn.preprocessing.PolynomialFeatures(1)\n",
        "# # df = poly.fit_transform(df)\n",
        "# # print(f\"df.shape={df.shape}\")\n",
        "\n",
        "# # apply a random projection - removed b/c this is adding too much noise\n",
        "\n",
        "# proj = sklearn.random_projection.GaussianRandomProjection(\n",
        "#     n_components=120,\n",
        "#     random_state=42,\n",
        "#     )\n",
        "\n",
        "# df = proj.fit_transform(df)\n",
        "# print(f\"df.shape={df.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK2G5GKvrUg2",
        "outputId": "fdc1680a-83fa-4676-f009-f7dc9d778f58"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df.shape=(1000, 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3: Create train/test sets"
      ],
      "metadata": {
        "id": "5u-wzRuBnVhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# STEP 3: Create train/test sets\n",
        "########################################\n",
        "\n",
        "train_ratio = 0.75\n",
        "validation_ratio = 0.15\n",
        "test_ratio = 0.10\n",
        "\n",
        "# ensure that the ratios sum to 1.0\n",
        "epsilon = 1e-10\n",
        "assert(1 - epsilon <= train_ratio + validation_ratio + test_ratio <= 1 + epsilon)\n",
        "\n",
        "# create train0/test set\n",
        "x_train0, x_test, y_train0, y_test = sklearn.model_selection.train_test_split(\n",
        "    df,\n",
        "    target,\n",
        "    test_size=test_ratio,\n",
        "    random_state=0,\n",
        "    )\n",
        "print(f\"len(x_train0)={len(x_train0)}\")\n",
        "print(f\"len(x_test)={len(x_test)}\")\n",
        "\n",
        "# create train/validation set\n",
        "x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(\n",
        "    x_train0,\n",
        "    y_train0,\n",
        "    test_size=validation_ratio/(train_ratio + validation_ratio),\n",
        "    random_state=0,\n",
        "    )\n",
        "print(f\"len(x_train)={len(x_train)}\")\n",
        "print(f\"len(x_val)={len(x_val)}\")"
      ],
      "metadata": {
        "id": "M3sOOgg6rVRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e418178-fdf4-4ac7-86a9-b1f3eceb17a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(x_train0)=900\n",
            "len(x_test)=100\n",
            "len(x_train)=750\n",
            "len(x_val)=150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 4-6: Create train/test sets"
      ],
      "metadata": {
        "id": "VtNNqaJcx6Q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# STEP 4: Apply \"learned\" data transformations\n",
        "########################################\n",
        "\n",
        "# scale the data\n",
        "scaler = sklearn.preprocessing.StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "x_train0 = scaler.transform(x_train0)\n",
        "x_train = scaler.transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "x_val = scaler.transform(x_val)\n",
        "print(f\"x_train0.shape={x_train0.shape}\")\n",
        "\n",
        "# PCA the data\n",
        "#pca = sklearn.decomposition.PCA(n_components=20)\n",
        "#pca.fit(x_train0)\n",
        "#x_train0 = pca.transform(x_train0)\n",
        "#x_train = pca.transform(x_train)\n",
        "#x_test = pca.transform(x_test)\n",
        "#x_val = pca.transform(x_val)\n",
        "#print(f\"x_train0.shape={x_train0.shape}\")"
      ],
      "metadata": {
        "id": "pl9B6u54rZWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f18007-f753-49ac-94a6-dffbc84ca805"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train0.shape=(900, 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# STEP 5: Train a model\n",
        "########################################\n",
        "\n",
        "# NOTE:\n",
        "# the models below are listed in the order we covered them in class;\n",
        "# the parameters are listed in the order of the documentation;\n",
        "# you are responsible for understanding how all specified parameters impact the runtime and/or statistical errors\n",
        "\n",
        "\n",
        "model = sklearn.tree.DecisionTreeClassifier(\n",
        "    criterion='gini',\n",
        "    max_depth=3, # decision stump\n",
        "    min_samples_split=4, # simplifying the decision tree does not improve performance, and after a certain point, decreases performance\n",
        "    min_samples_leaf=5,\n",
        "    max_features=None,\n",
        "    max_leaf_nodes=50,\n",
        "    random_state=42,\n",
        "    )\n",
        "model = sklearn.ensemble.AdaBoostClassifier(\n",
        "    estimator=model,\n",
        "    learning_rate=0.5,\n",
        "    n_estimators=17, # optimal number of estimators for performance, lots more cause overfitting or hurt performance\n",
        "    )\n",
        "\n",
        "# model = sklearn.linear_model.LogisticRegression(\n",
        "#     C=1e1,\n",
        "#     penalty='l2',\n",
        "#     dual=False,\n",
        "#     fit_intercept=True,\n",
        "#     intercept_scaling=True,\n",
        "#     max_iter=100,\n",
        "#     tol=1e-3,\n",
        "#     random_state=42,\n",
        "#     verbose=1,\n",
        "#     )\n",
        "\n",
        "# quality over quantity for this ensemble -> (complex decisiontreeclassifier, simple AdaBoost)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# report validation accuracy\n",
        "validation_accuracy = model.score(x_val, y_val)\n",
        "print(f\"validation_accuracy={validation_accuracy:0.4f}\") #this is like Eout\n",
        "train_accuracy = model.score(x_train, y_train)\n",
        "print(f\"train_accuracy={train_accuracy:0.4f}\") # this is like Ein"
      ],
      "metadata": {
        "id": "9Q5Q_lQyrurD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03ea85c3-d56d-41b0-fc52-e2e8e45386bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation_accuracy=0.6467\n",
            "train_accuracy=0.7400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# STEP 6: Evaluate on test set\n",
        "########################################\n",
        "\n",
        "# WARNING:\n",
        "# this code should be run only once;\n",
        "# after the hyperparameters have been decided based on the validation performance,\n",
        "# then the False can be changed to True to run this code\n",
        "if True:\n",
        "    model.fit(x_train0, y_train0)\n",
        "    test_accuracy = model.score(x_test, y_test)\n",
        "    print(f\"test_accuracy={test_accuracy}\")"
      ],
      "metadata": {
        "id": "wRVpp8XPrwJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f23f7832-7515-460d-8f6d-e62266e58017"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_accuracy=0.73\n"
          ]
        }
      ]
    }
  ]
}